# üìã FASE 0: SETUP & FUNDA√á√ÉO - DOCUMENTA√á√ÉO COMPLETA

**Dura√ß√£o:** 7 dias | **Status:** ‚úÖ COMPLETA | **Data:** 27/01/2026

---

## üéØ OBJETIVO

Configurar ambiente de desenvolvimento local com:
- Airflow rodando em Docker
- Google Cloud Platform integrado
- Dataset Olist baixado
- Schema PostgreSQL criado e validado

---

## üìä ENTREGAS

| Item | Status | Valida√ß√£o |
|------|--------|-----------|
| Reposit√≥rio GitHub | ‚úÖ | `git remote -v` |
| Airflow 2.8.1 rodando | ‚úÖ | http://localhost:8080 |
| GCP configurado | ‚úÖ | Bucket + Dataset BigQuery |
| CSVs Olist (9 arquivos) | ‚úÖ | `data/raw/` (550k registros) |
| PostgreSQL schema (9 tabelas) | ‚úÖ | DAG 04_create_schema |
| 3 DAGs funcionando | ‚úÖ | Todos executaram com sucesso |

---

## üõ†Ô∏è TECNOLOGIAS UTILIZADAS
```yaml
Orquestra√ß√£o:
  - Apache Airflow 2.8.1
  - Docker & Docker Compose

Banco de Dados:
  - PostgreSQL 13 (local, Docker)
  - Google BigQuery (cloud)

Cloud:
  - Google Cloud Platform
  - Cloud Storage (data lake)
  - Vertex AI (preparado)

Linguagens:
  - Python 3.10
  - SQL (PostgreSQL dialect)

Versionamento:
  - Git
  - GitHub
```

---

## üìÅ ESTRUTURA DE PASTAS CRIADA
```
olist-data-pipeline/
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03_test_olist_csv.py      ‚úÖ Testa leitura CSVs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 04_create_schema.py       ‚úÖ Cria schema PostgreSQL
‚îÇ   ‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ plugins/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ database/
‚îÇ       ‚îî‚îÄ‚îÄ schema_clean.sql          ‚úÖ Schema 9 tabelas
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ raw/
‚îÇ       ‚îî‚îÄ‚îÄ *.csv                     ‚úÖ 9 CSVs Olist (550k registros)
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ PHASE_0_SETUP.md              ‚úÖ Esta documenta√ß√£o
‚îú‚îÄ‚îÄ docker-compose.yml                ‚úÖ Airflow + PostgreSQL
‚îú‚îÄ‚îÄ requirements.txt                  ‚úÖ Depend√™ncias Python
‚îú‚îÄ‚îÄ .env                              ‚úÖ Vari√°veis ambiente (GCP)
‚îú‚îÄ‚îÄ .gitignore                        ‚úÖ Seguran√ßa (credenciais)
‚îú‚îÄ‚îÄ gcp-credentials.json              üîí N√ÉO versionado
‚îî‚îÄ‚îÄ README.md                         ‚úÖ Overview projeto
```

---

## üîß CONFIGURA√á√ïES REALIZADAS

### **1. Docker Compose**

**Arquivo:** `docker-compose.yml`

**Servi√ßos configurados:**
- `postgres`: PostgreSQL 13 (porta 5432)
- `airflow-webserver`: UI (porta 8080)
- `airflow-scheduler`: Executor de DAGs
- `airflow-init`: Inicializador

**Volumes montados:**
```yaml
- ./airflow/dags:/opt/airflow/dags
- ./src:/opt/airflow/src
- ./data:/opt/airflow/data
- ./gcp-credentials.json:/opt/airflow/gcp-credentials.json:ro
```

**Credenciais Airflow:**
- User: `admin`
- Password: `admin`

---

### **2. Google Cloud Platform**

**Projeto:** `olist-data-platform`

**Recursos criados:**
1. **Service Account:** `airflow-olist@olist-data-platform.iam.gserviceaccount.com`
   - Roles: BigQuery Admin, Storage Admin, Vertex AI User, Service Account User, Logging Viewer, Monitoring Viewer

2. **Cloud Storage Bucket:** `olist-data-lake-hyego`
   - Regi√£o: `us-central1`
   - Classe: Standard
   - Controle acesso: Uniform

3. **BigQuery Dataset:** `olist_analytics`
   - Regi√£o: US (m√∫ltiplas regi√µes)
   - Expiration: Nunca

**Vari√°veis ambiente (.env):**
```env
GCP_PROJECT_ID=olist-data-platform
GCP_BUCKET_NAME=gs://olist-data-lake-hyego
GCP_DATASET_ID=olist_analytics
GCP_SERVICE_ACCOUNT_KEY_PATH=/opt/airflow/gcp-credentials.json
```

---

### **3. Airflow Connection**

**Connection ID:** `postgres_default`
```yaml
Connection Type: Postgres
Host: postgres
Database: airflow
Login: airflow
Password: airflow
Port: 5432
```

**Como configurar:**
1. Admin ‚Üí Connections
2. Add (+)
3. Preencher campos acima
4. Save

---

## üì¶ DATASET OLIST

**Fonte:** [Kaggle - Brazilian E-Commerce](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

**Arquivos (9 CSVs):**
1. `olist_customers_dataset.csv` (99.441 linhas)
2. `olist_sellers_dataset.csv` (3.095 linhas)
3. `olist_products_dataset.csv` (32.951 linhas)
4. `olist_geolocation_dataset.csv` (1.000.163 linhas)
5. `olist_orders_dataset.csv` (99.441 linhas)
6. `olist_order_items_dataset.csv` (112.650 linhas)
7. `olist_order_payments_dataset.csv` (103.886 linhas)
8. `olist_order_reviews_dataset.csv` (99.224 linhas)
9. `product_category_name_translation.csv` (71 linhas)

**Total de registros:** ~550.000

**Localiza√ß√£o:** `data/raw/`

---

## üóÑÔ∏è SCHEMA POSTGRESQL

**Arquivo:** `src/database/schema_clean.sql`

**Tabelas criadas (9):**

### **Tabelas independentes (sem FK):**
1. **customers** (PK: customer_id)
2. **sellers** (PK: seller_id)
3. **products** (PK: product_id)
4. **product_category_name_translation** (sem PK)
5. **geolocation** (sem PK)

### **Tabelas dependentes (com FK):**
6. **orders** (PK: order_id, FK: customer_id)
7. **order_items** (PK composta: order_id + order_item_id, FKs: order_id, product_id, seller_id)
8. **order_payments** (PK composta: order_id + payment_sequential, FK: order_id)
9. **order_reviews** (PK: review_pk SERIAL, FK: order_id, UNIQUE: review_id)

**Integridade referencial:** 100% (0 √≥rf√£os)

**Constraints:**
- Primary Keys: 7
- Foreign Keys: 6
- Unique: 1
- Serial (auto-increment): 1

---

## üöÄ DAGS CRIADOS

### **DAG 1: 03_test_olist_csv**

**Objetivo:** Validar leitura dos CSVs do Olist

**Tasks:**
- `test_read_olist_csv`: L√™ 5 linhas de 3 CSVs principais

**C√≥digo:**
```python
# L√™ CSVs com pandas
df = pd.read_csv(file_path, nrows=5)
print(f"‚úÖ {csv_file}: {df.shape}")
```

**Resultado:** ‚úÖ Sucesso (3 CSVs lidos)

---

### **DAG 2: 04_create_schema**

**Objetivo:** Criar schema PostgreSQL (9 tabelas)

**Tasks:**
1. `create_tables`: Executa SQL para criar tabelas
2. `validate_tables`: Valida que 9 tabelas existem

**C√≥digo principal:**
```python
def create_tables(**context):
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    sql_content = open('/opt/airflow/src/database/schema_clean.sql').read()
    pg_hook.run(sql_content)
```

**Valida√ß√£o:**
```sql
SELECT table_name 
FROM information_schema.tables 
WHERE table_schema = 'public';
```

**Resultado:** ‚úÖ Sucesso (9 tabelas criadas e validadas)

---

## ‚úÖ VALIDA√á√ïES REALIZADAS

### **1. Teste de conex√£o GCP**
```bash
docker-compose exec airflow-webserver python /opt/airflow/src/test_gcp_connection.py
```

**Resultado:**
```
‚úÖ Cloud Storage: 1 bucket encontrado
‚úÖ BigQuery: 1 dataset encontrado
```

---

### **2. Verifica√ß√£o estrutura de pastas**
```bash
docker-compose exec airflow-webserver ls -la /opt/airflow/src/database/
```

**Resultado:**
```
-rwxrwxrwx 1 root root 3592 schema_clean.sql
```

---

### **3. Valida√ß√£o schema PostgreSQL**

**Query executada:**
```sql
SELECT table_name 
FROM information_schema.tables 
WHERE table_schema = 'public'
ORDER BY table_name;
```

**Resultado esperado (9 tabelas):**
```
customers
geolocation
order_items
order_payments
order_reviews
orders
product_category_name_translation
products
sellers
```

---

## üêõ PROBLEMAS ENCONTRADOS E SOLU√á√ïES

### **Problema 1: DAG n√£o encontrava arquivo SQL**

**Erro:**
```
jinja2.exceptions.TemplateNotFound: src/database/schema_clean.sql
```

**Causa:** `PostgresOperator` tentava usar path como template Jinja2

**Solu√ß√£o:** Ler arquivo com Python e executar SQL via `PostgresHook`
```python
# ANTES (errado)
PostgresOperator(sql='src/database/schema_clean.sql')

# DEPOIS (correto)
sql_content = open('/opt/airflow/src/database/schema_clean.sql').read()
pg_hook.run(sql_content)
```

---

### **Problema 2: Connection n√£o configurada**

**Erro:**
```
AirflowNotFoundException: The conn_id `postgres_default` isn't defined
```

**Causa:** Airflow sem conex√£o configurada para PostgreSQL

**Solu√ß√£o:** Criar connection via UI (Admin ‚Üí Connections)

---

### **Problema 3: Volume n√£o montado inicialmente**

**Erro:** Arquivo existia no Windows mas n√£o no container

**Causa:** Docker n√£o estava vendo pasta `src/`

**Solu√ß√£o:** Verificar `docker-compose.yml` e reiniciar containers
```bash
docker-compose down
docker-compose up -d
```

---

## üìö CONCEITOS APLICADOS

### **Airflow:**
- DAG (Directed Acyclic Graph)
- Operators (PythonOperator, PostgresOperator)
- Hooks (PostgresHook)
- Connections (gerenciamento credenciais)
- Task dependencies (`>>`)

### **Docker:**
- Containers
- Volumes (bind mounts)
- docker-compose
- Networks

### **SQL:**
- DDL (CREATE TABLE)
- Constraints (PK, FK, UNIQUE)
- SERIAL (auto-increment)
- Referential integrity

### **GCP:**
- Service Accounts
- IAM Roles
- Cloud Storage (buckets)
- BigQuery (datasets)

---

## üéì SKILLS DEMONSTRADAS

| Skill | N√≠vel | Evid√™ncia |
|-------|-------|-----------|
| Data Modeling | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 9 tabelas, PKs compostas, FKs |
| SQL (DDL) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Schema completo, constraints |
| Apache Airflow | ‚≠ê‚≠ê‚≠ê‚≠ê | 2 DAGs, operators, hooks |
| Docker | ‚≠ê‚≠ê‚≠ê | docker-compose, volumes |
| GCP | ‚≠ê‚≠ê‚≠ê‚≠ê | Service account, IAM, buckets |
| Python | ‚≠ê‚≠ê‚≠ê‚≠ê | Pandas, PostgresHook |
| Git | ‚≠ê‚≠ê‚≠ê | Versionamento, .gitignore |
| Troubleshooting | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Debug erros Jinja2, volumes |

---

## üìä M√âTRICAS DA FASE
```
Tempo investido: 6-8 horas
Linhas de c√≥digo: ~200
Arquivos criados: 10+
Commits Git: 5
Erros resolvidos: 3
DAGs funcionando: 2
Tabelas criadas: 9
Registros dataset: 550.000+
```

---

## üîÑ PR√ìXIMA FASE

**FASE 1: Data Ingestion (Semanas 2-3)**

**Objetivos:**
- DAG: CSV ‚Üí PostgreSQL (9 tabelas populadas)
- DAG: CSV ‚Üí GCS (data lake bronze layer)
- DAG: Data validation (Great Expectations)

**Prepara√ß√£o:**
- Schema PostgreSQL: ‚úÖ Pronto
- CSVs Olist: ‚úÖ Prontos
- GCS Bucket: ‚úÖ Criado

---

## üìù COMMITS RECOMENDADOS
```bash
git add .
git commit -m "docs: complete Phase 0 documentation"
git push origin main
```

---

## üÜò TROUBLESHOOTING R√ÅPIDO

**Airflow n√£o sobe:**
```bash
docker-compose down -v
docker-compose up airflow-init
docker-compose up -d
```

**DAG n√£o aparece:**
- Aguardar 30s
- F5 no browser
- Verificar logs: `docker-compose logs airflow-scheduler`

**Erro de conex√£o PostgreSQL:**
- Admin ‚Üí Connections
- Validar postgres_default existe
- Test connection

---

**Documenta√ß√£o gerada em:** 27/01/2026  
**Autor:** Hyego Jarllys  
**Projeto:** Olist Data Platform  
**Status:** ‚úÖ Fase 0 Completa