# OLIST DATA PLATFORM - ARCHITECTURE DOCUMENTATION

**Projeto:** Olist E-commerce Data Platform  
**Autor:** Hyego Jarllys  
**Data:** Janeiro 2025  
**VersÃ£o:** 1.0  
**Status:** Production  

---

## ğŸ“‹ ÃNDICE

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [Arquitetura de Alto NÃ­vel](#arquitetura-de-alto-nÃ­vel)
3. [Componentes TÃ©cnicos](#componentes-tÃ©cnicos)
4. [Fluxo de Dados](#fluxo-de-dados)
5. [Infraestrutura](#infraestrutura)
6. [SeguranÃ§a](#seguranÃ§a)
7. [Escalabilidade](#escalabilidade)
8. [Disaster Recovery](#disaster-recovery)
9. [Diagramas TÃ©cnicos](#diagramas-tÃ©cnicos)

---

## ğŸ¯ VISÃƒO GERAL

### Objetivo da Arquitetura

A arquitetura do Olist Data Platform foi projetada seguindo princÃ­pios modernos de engenharia de dados:

- **Modularidade:** Componentes independentes e substituÃ­veis
- **Escalabilidade:** Capacidade de crescer horizontalmente
- **Observabilidade:** Monitoramento em todas as camadas
- **ResiliÃªncia:** TolerÃ¢ncia a falhas e recuperaÃ§Ã£o automÃ¡tica
- **Simplicidade:** Minimal viable architecture (MVA) para MVP

### PrincÃ­pios de Design

1. **Separation of Concerns**
   - OrquestraÃ§Ã£o (Airflow) separada de armazenamento (PostgreSQL/GCS)
   - Camadas de dados claramente definidas (bronze/silver/gold)
   - ValidaÃ§Ã£o de qualidade independente do processamento

2. **Infrastructure as Code**
   - Docker Compose para reprodutibilidade
   - ConfiguraÃ§Ãµes versionadas
   - Ambientes idÃªnticos (dev/staging/prod)

3. **Data Contracts**
   - Schemas explÃ­citos (DDL versionado)
   - Great Expectations como contrato de qualidade
   - DocumentaÃ§Ã£o como cÃ³digo

4. **Fail Fast**
   - ValidaÃ§Ãµes antes de processar
   - Rollback automÃ¡tico em caso de falha
   - Alertas proativos

---

## ğŸ—ï¸ ARQUITETURA DE ALTO NÃVEL

### Camadas da Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PRESENTATION LAYER                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Airflow UI  â”‚  â”‚  Data Docs   â”‚  â”‚   pgAdmin    â”‚          â”‚
â”‚  â”‚  (port 8080) â”‚  â”‚    (HTML)    â”‚  â”‚  (future)    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ORCHESTRATION LAYER                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Apache Airflow 2.8.1                        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚ Webserver  â”‚  â”‚ Scheduler  â”‚  â”‚   LocalExecutor  â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚         13 DAGs (Python)    +    Task Dependencies      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PROCESSING LAYER                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Pandas     â”‚  â”‚  SQLAlchemy  â”‚  â”‚Great Expect. â”‚          â”‚
â”‚  â”‚ (Transform)  â”‚  â”‚   (I/O DB)   â”‚  â”‚ (Validation) â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       STORAGE LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   PostgreSQL 13        â”‚  â”‚   Google Cloud Storage       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ olist_raw       â”‚   â”‚  â”‚  â”‚ bronze/                â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  - customers    â”‚   â”‚  â”‚  â”‚  - customers.parquet   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  - orders       â”‚   â”‚  â”‚  â”‚  - orders.parquet      â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  - products     â”‚   â”‚  â”‚  â”‚  - products.parquet    â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  - sellers      â”‚   â”‚  â”‚  â”‚  - sellers.parquet     â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  - order_items  â”‚   â”‚  â”‚  â”‚  - order_items.parquet â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  - payments     â”‚   â”‚  â”‚  â”‚  - payments.parquet    â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  - reviews      â”‚   â”‚  â”‚  â”‚  - reviews.parquet     â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  - geolocation  â”‚   â”‚  â”‚  â”‚  - geolocation.parquet â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚  850k+ registros       â”‚  â”‚  ~20 MB total              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SOURCE LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  CSV Files (Local)                       â”‚   â”‚
â”‚  â”‚  olist_customers_dataset.csv         (99.441 linhas)    â”‚   â”‚
â”‚  â”‚  olist_orders_dataset.csv            (99.441 linhas)    â”‚   â”‚
â”‚  â”‚  olist_order_items_dataset.csv       (112.650 linhas)   â”‚   â”‚
â”‚  â”‚  olist_order_payments_dataset.csv    (103.886 linhas)   â”‚   â”‚
â”‚  â”‚  olist_order_reviews_dataset.csv     (99.224 linhas)    â”‚   â”‚
â”‚  â”‚  olist_products_dataset.csv          (32.951 linhas)    â”‚   â”‚
â”‚  â”‚  olist_sellers_dataset.csv           (3.095 linhas)     â”‚   â”‚
â”‚  â”‚  olist_geolocation_dataset.csv       (1M+ linhas)       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PadrÃ£o Arquitetural: Medallion Architecture

A arquitetura segue o padrÃ£o **Medallion** (bronze/silver/gold) da Databricks:

**Bronze Layer (Raw Data):**
- Dados brutos, sem transformaÃ§Ãµes
- Formato: Parquet (compressÃ£o, schema)
- Storage: Google Cloud Storage
- Objetivo: Data lake para auditoria e reprocessamento

**Silver Layer (Cleaned Data) - FASE 2:**
- Dados limpos, validados, normalizados
- Formato: Parquet particionado
- Storage: GCS + PostgreSQL (tabelas otimizadas)
- Objetivo: Analytics-ready data

**Gold Layer (Business Data) - FASE 2:**
- AgregaÃ§Ãµes, mÃ©tricas de negÃ³cio
- Formato: Tabelas relacionais otimizadas
- Storage: PostgreSQL (star schema)
- Objetivo: Dashboards e relatÃ³rios

---

## ğŸ”§ COMPONENTES TÃ‰CNICOS

### 1. Apache Airflow

**VersÃ£o:** 2.8.1  
**Executor:** LocalExecutor  
**Backend:** PostgreSQL  

**Componentes:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Apache Airflow Architecture             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Airflow Webserver                â”‚  â”‚
â”‚  â”‚  - Flask application (port 8080)         â”‚  â”‚
â”‚  â”‚  - UI para monitoramento                 â”‚  â”‚
â”‚  â”‚  - AutenticaÃ§Ã£o bÃ¡sica (admin/admin)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                           â”‚
â”‚                     â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Airflow Scheduler                â”‚  â”‚
â”‚  â”‚  - Parse DAGs (cada 30s)                 â”‚  â”‚
â”‚  â”‚  - Cria DagRuns e TaskInstances          â”‚  â”‚
â”‚  â”‚  - Enfileira tasks                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                           â”‚
â”‚                     â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         LocalExecutor                    â”‚  â”‚
â”‚  â”‚  - Executa tasks em subprocessos         â”‚  â”‚
â”‚  â”‚  - Paralelismo configurÃ¡vel              â”‚  â”‚
â”‚  â”‚  - Logs em arquivos                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                           â”‚
â”‚                     â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Metadata Database                â”‚  â”‚
â”‚  â”‚  - PostgreSQL (mesmo container)          â”‚  â”‚
â”‚  â”‚  - Schema: airflow                       â”‚  â”‚
â”‚  â”‚  - Tabelas: dag, dag_run, task_instance  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ConfiguraÃ§Ãµes Importantes:**

```yaml
AIRFLOW__CORE__EXECUTOR: LocalExecutor
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: true
AIRFLOW__CORE__LOAD_EXAMPLES: false
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
```

**DAG Structure Pattern:**

```python
with DAG(
    dag_id='XX_<action>_<entity>',
    default_args={
        'owner': 'hyego',
        'retries': 2,
        'retry_delay': timedelta(minutes=5)
    },
    schedule_interval=None,  # Manual trigger
    catchup=False,
    tags=['fase-1', 'category']
) as dag:
    
    validate_task >> load_task >> quality_task
```

### 2. PostgreSQL

**VersÃ£o:** 13  
**Role:** Transactional database + Airflow metadata  

**Schemas:**

```sql
-- Airflow metadata (auto-criado)
airflow
  â”œâ”€â”€ dag
  â”œâ”€â”€ dag_run
  â”œâ”€â”€ task_instance
  â””â”€â”€ ... (40+ tabelas)

-- Dados do projeto
olist_raw
  â”œâ”€â”€ customers (99.441 registros)
  â”œâ”€â”€ sellers (3.095 registros)
  â”œâ”€â”€ products (32.951 registros)
  â”œâ”€â”€ orders (99.441 registros)
  â”œâ”€â”€ order_items (112.650 registros)
  â”œâ”€â”€ order_payments (103.886 registros)
  â”œâ”€â”€ order_reviews (99.224 registros)
  â””â”€â”€ geolocation (19.000 registros)
```

**ConfiguraÃ§Ãµes:**

```yaml
POSTGRES_USER: airflow
POSTGRES_PASSWORD: airflow
POSTGRES_DB: airflow
Port: 5432
Max Connections: 100 (default)
Shared Buffers: 128MB (default)
```

**Ãndices Criados:**

Total: 28 Ã­ndices

- Primary Keys: 8 (1 por tabela)
- Foreign Keys: 6 Ã­ndices automÃ¡ticos
- Custom Indexes: 14 (em colunas de filtro/join)

**Exemplo:**
```sql
CREATE INDEX idx_orders_customer ON olist_raw.orders(customer_id);
CREATE INDEX idx_orders_status ON olist_raw.orders(order_status);
CREATE INDEX idx_orders_purchase_date ON olist_raw.orders(order_purchase_timestamp);
```

### 3. Google Cloud Storage

**Bucket:** `olist-data-lake-hyego`  
**Region:** us-central1 (configurÃ¡vel)  
**Storage Class:** Standard  

**Estrutura de DiretÃ³rios:**

```
gs://olist-data-lake-hyego/
â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ customers/
â”‚   â”‚   â””â”€â”€ 2025-01-28.parquet (metadados: _loaded_at, _source_file)
â”‚   â”œâ”€â”€ sellers/
â”‚   â”‚   â””â”€â”€ 2025-01-28.parquet
â”‚   â”œâ”€â”€ products/
â”‚   â”‚   â””â”€â”€ 2025-01-28.parquet
â”‚   â”œâ”€â”€ orders/
â”‚   â”‚   â””â”€â”€ 2025-01-28.parquet
â”‚   â”œâ”€â”€ order_items/
â”‚   â”‚   â””â”€â”€ 2025-01-28.parquet
â”‚   â”œâ”€â”€ order_payments/
â”‚   â”‚   â””â”€â”€ 2025-01-28.parquet
â”‚   â”œâ”€â”€ order_reviews/
â”‚   â”‚   â””â”€â”€ 2025-01-28.parquet
â”‚   â””â”€â”€ geolocation/
â”‚       â””â”€â”€ 2025-01-28.parquet
â”œâ”€â”€ silver/ (Fase 2)
â””â”€â”€ gold/ (Fase 2)
```

**Parquet Schema:**

Cada arquivo Parquet contÃ©m:
- Todas as colunas do CSV original
- Tipos de dados preservados (int, float, timestamp, string)
- Metadados adicionais:
  - `_loaded_at`: TIMESTAMP (quando foi carregado)
  - `_source_file`: STRING (nome do CSV original)

**ConfiguraÃ§Ã£o de Acesso:**

```python
# Credenciais via Service Account
GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/gcp-credentials.json

# Bibliotecas
google-cloud-storage==2.14.0
google-auth>=2.23.3
```

### 4. Great Expectations

**VersÃ£o:** 0.18.8  
**Context Type:** FileDataContext  
**Location:** `/opt/airflow/great_expectations` (volume persistido)  

**Estrutura de Arquivos:**

```
great_expectations/
â”œâ”€â”€ great_expectations.yml           # Config principal
â”œâ”€â”€ expectations/                    # Suites de validaÃ§Ã£o
â”‚   â”œâ”€â”€ orders_suite.json           # 11 expectations
â”‚   â”œâ”€â”€ customers_suite.json        # 10 expectations
â”‚   â””â”€â”€ order_items_suite.json      # 10 expectations
â”œâ”€â”€ checkpoints/                     # Pontos de execuÃ§Ã£o
â”‚   â”œâ”€â”€ orders_checkpoint.yml
â”‚   â”œâ”€â”€ customers_checkpoint.yml
â”‚   â””â”€â”€ order_items_checkpoint.yml
â”œâ”€â”€ plugins/                         # Custom expectations (vazio)
â””â”€â”€ uncommitted/                     # NÃ£o versionado
    â”œâ”€â”€ data_docs/                   # HTML gerado
    â”‚   â””â”€â”€ local_site/
    â”‚       â”œâ”€â”€ index.html
    â”‚       â”œâ”€â”€ expectations/
    â”‚       â””â”€â”€ validations/
    â””â”€â”€ validations/                 # Resultados das execuÃ§Ãµes
        â””â”€â”€ orders_suite/
            â””â”€â”€ orders/
                â””â”€â”€ 20250129-*.json
```

**Datasource Configuration:**

```python
datasource_config = {
    "name": "postgres_datasource",
    "class_name": "Datasource",
    "execution_engine": {
        "class_name": "SqlAlchemyExecutionEngine",
        "connection_string": "postgresql://airflow:airflow@postgres:5432/airflow"
    },
    "data_connectors": {
        "default_runtime_data_connector": {
            "class_name": "RuntimeDataConnector",
            "batch_identifiers": ["default_identifier_name"]
        }
    }
}
```

**Checkpoint Pattern:**

```python
checkpoint_config = {
    "name": "table_checkpoint",
    "config_version": 1.0,
    "class_name": "SimpleCheckpoint",
    "run_name_template": "%Y%m%d-%H%M%S-table-validation",
}

results = context.run_checkpoint(
    checkpoint_name="table_checkpoint",
    validations=[{
        "batch_request": batch_request,
        "expectation_suite_name": "table_suite"
    }]
)
```

### 5. Docker & Docker Compose

**Docker Version:** 20.10+  
**Docker Compose Version:** 2.x  

**ServiÃ§os Definidos:**

```yaml
services:
  postgres:
    image: postgres:13
    ports: ["5432:5432"]
    volumes: [postgres-db-volume:/var/lib/postgresql/data]
    
  airflow-webserver:
    build: .
    command: webserver
    ports: ["8080:8080"]
    depends_on: [postgres, airflow-init]
    
  airflow-scheduler:
    build: .
    command: scheduler
    depends_on: [postgres, airflow-init]
    
  airflow-init:
    build: .
    command: airflow version
    environment:
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
```

**Volumes Montados:**

```yaml
volumes:
  - ./airflow/dags:/opt/airflow/dags
  - ./airflow/logs:/opt/airflow/logs
  - ./data:/opt/airflow/data
  - ./great_expectations:/opt/airflow/great_expectations
  - ./gcp-credentials.json:/opt/airflow/gcp-credentials.json:ro
```

**Dockerfile Customizado:**

```dockerfile
FROM apache/airflow:2.8.1-python3.10

USER root
RUN apt-get update && apt-get install -y --no-install-recommends \
    && apt-get clean

USER airflow
COPY requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt

RUN mkdir -p /opt/airflow/great_expectations
```

---

## ğŸ”„ FLUXO DE DADOS

### Fluxo End-to-End

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASE 1: DATA INGESTION (Implementado)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. SOURCE â†’ VALIDATION
   CSV Files (local)
      â”‚
      â–¼
   [validate_csv task]
      â”œâ”€ Verificar colunas esperadas
      â”œâ”€ Validar PKs nÃ£o nulas
      â”œâ”€ Validar FKs nÃ£o nulas
      â”œâ”€ Detectar duplicatas
      â””â”€ Gerar estatÃ­sticas

2. VALIDATION â†’ POSTGRESQL
   [load_to_postgres task]
      â”œâ”€ Remover duplicatas (drop_duplicates)
      â”œâ”€ Converter timestamps (pd.to_datetime)
      â”œâ”€ TRUNCATE table (limpar dados antigos)
      â”œâ”€ INSERT com chunks (1000 registros/batch)
      â””â”€ Validar row count

3. POSTGRESQL â†’ QUALITY CHECK
   [validate_data_quality task]
      â”œâ”€ Queries agregadas (COUNT, AVG, SUM)
      â”œâ”€ Validar Foreign Keys (0 Ã³rfÃ£os)
      â”œâ”€ DistribuiÃ§Ãµes (ex: status de orders)
      â””â”€ Log de mÃ©tricas

4. SOURCE â†’ GCS (paralelo)
   CSV Files
      â”‚
      â–¼
   [csv_to_parquet_gcs task]
      â”œâ”€ Ler CSV com Pandas
      â”œâ”€ Adicionar metadados (_loaded_at, _source_file)
      â”œâ”€ Converter para Parquet (pyarrow)
      â”œâ”€ Upload para GCS (google-cloud-storage)
      â””â”€ Validar upload

5. POSTGRESQL â†’ GREAT EXPECTATIONS
   PostgreSQL tables
      â”‚
      â–¼
   [run_great_expectations_validations]
      â”œâ”€ Criar Runtime Batch (LIMIT 10k)
      â”œâ”€ Executar Expectation Suite
      â”œâ”€ Gerar Validation Results
      â”œâ”€ Build Data Docs
      â””â”€ Log success rate
```

### Fluxo de uma DAG de IngestÃ£o (Exemplo: orders)

```
START
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  validate_csv                       â”‚
â”‚  - Ler: olist_orders_dataset.csv   â”‚
â”‚  - Validar: 8 colunas esperadas    â”‚
â”‚  - PKs nulas: 0                    â”‚
â”‚  - FKs nulas: 0 (customer_id)      â”‚
â”‚  - Duplicatas: X removidas         â”‚
â”‚  - Output: Log estatÃ­sticas        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚ SUCCESS
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  load_to_postgres                   â”‚
â”‚  - Conectar: SQLAlchemy engine      â”‚
â”‚  - TRUNCATE olist_raw.orders        â”‚
â”‚  - Converter timestamps (4 cols)    â”‚
â”‚  - INSERT 99.441 registros          â”‚
â”‚  - Chunks: 1000 registros/batch    â”‚
â”‚  - Validar COUNT(*) = 99.441       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚ SUCCESS
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  validate_data_quality              â”‚
â”‚  - Query: COUNT DISTINCT order_id   â”‚
â”‚  - Query: COUNT por order_status    â”‚
â”‚  - Query: FK Ã³rfÃ£os (orders â†’ cust) â”‚
â”‚  - Resultado: 0 Ã³rfÃ£os              â”‚
â”‚  - Output: MÃ©tricas agregadas       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚ SUCCESS
  â–¼
END (DAG Success)
```

### Fluxo de ValidaÃ§Ã£o Great Expectations

```
START
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Carregar FileDataContext            â”‚
â”‚  - Path: /opt/airflow/great_expect.. â”‚
â”‚  - Config: great_expectations.yml    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Criar Datasource (PostgreSQL)       â”‚
â”‚  - Connection string                 â”‚
â”‚  - RuntimeDataConnector              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Criar Batch Request                 â”‚
â”‚  - Query: SELECT * FROM table LIMIT  â”‚
â”‚  - Batch identifier                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Executar Checkpoint                 â”‚
â”‚  - Load Expectation Suite            â”‚
â”‚  - Run validations                   â”‚
â”‚  - Generate results JSON             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Parse Results                       â”‚
â”‚  - success: true/false               â”‚
â”‚  - statistics.success_percent        â”‚
â”‚  - evaluated_expectations            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Build Data Docs                     â”‚
â”‚  - Generate HTML pages               â”‚
â”‚  - Create index.html                 â”‚
â”‚  - Save to uncommitted/data_docs/    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
END
```

---

## ğŸ–¥ï¸ INFRAESTRUTURA

### Ambientes

**Desenvolvimento (Atual):**
- Local machine (Windows)
- Docker Desktop
- Recursos: 4 CPU, 8GB RAM
- Storage: SSD local

**ProduÃ§Ã£o (Futuro - Recomendado):**
- Google Cloud Platform
- Cloud Composer (Airflow gerenciado)
- Cloud SQL (PostgreSQL gerenciado)
- Networking: VPC privada

### Requisitos de Sistema

**MÃ­nimo:**
- CPU: 2 cores
- RAM: 4 GB
- Disk: 20 GB SSD
- Network: 10 Mbps

**Recomendado:**
- CPU: 4 cores
- RAM: 8 GB
- Disk: 50 GB SSD
- Network: 50 Mbps

**Para ProduÃ§Ã£o:**
- CPU: 8+ cores
- RAM: 16+ GB
- Disk: 100+ GB SSD (com backup)
- Network: 100+ Mbps

### Network Topology

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Docker Network: olist-data-pipeline_default      â”‚
â”‚  Driver: bridge                                    â”‚
â”‚  Subnet: 172.x.0.0/16 (auto-assigned)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Webserver   â”‚â—„â”€â”€â”€â”€â–ºâ”‚  Scheduler   â”‚          â”‚
â”‚  â”‚  172.x.0.2   â”‚      â”‚  172.x.0.3   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚         â”‚                     â”‚                   â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                   â–¼                               â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚         â”‚    PostgreSQL    â”‚                      â”‚
â”‚         â”‚    172.x.0.4     â”‚                      â”‚
â”‚         â”‚  hostname: postgres                     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (Port forwarding)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Host Machine                                      â”‚
â”‚  localhost:8080 â†’ Webserver:8080                  â”‚
â”‚  localhost:5432 â†’ PostgreSQL:5432                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ComunicaÃ§Ã£o entre ServiÃ§os

| From | To | Protocol | Port | Purpose |
|------|-----|----------|------|---------|
| Webserver | PostgreSQL | TCP | 5432 | Metastore queries |
| Scheduler | PostgreSQL | TCP | 5432 | DAG metadata |
| Webserver | Scheduler | HTTP | 8974 | Health checks |
| Host | Webserver | HTTP | 8080 | UI access |
| Host | PostgreSQL | TCP | 5432 | Direct queries (optional) |
| Airflow | GCS | HTTPS | 443 | Upload Parquet files |

---

## ğŸ”’ SEGURANÃ‡A

### AutenticaÃ§Ã£o e AutorizaÃ§Ã£o

**Airflow UI:**
- MÃ©todo: Basic Auth (username/password)
- Default user: `admin` / `admin`
- RecomendaÃ§Ã£o prod: integrar com LDAP/OAuth2

**PostgreSQL:**
- User: `airflow`
- Password: `airflow` (env variable)
- Acesso: limitado Ã  rede Docker
- RecomendaÃ§Ã£o prod: passwords complexos via Secret Manager

**Google Cloud:**
- MÃ©todo: Service Account
- File: `gcp-credentials.json`
- Permissions: Storage Object Admin
- Acesso: read-only mount no container

### Secrets Management

**Atual (Dev):**
```yaml
environment:
  POSTGRES_PASSWORD: airflow  # Plain text (OK para dev)
  AIRFLOW__CORE__FERNET_KEY: '' # Vazio (sem encryption)
```

**Recomendado (Prod):**
```python
# Usar Airflow Connections
from airflow.hooks.base import BaseHook

postgres_conn = BaseHook.get_connection('postgres_default')
gcs_conn = BaseHook.get_connection('google_cloud_default')

# Ou GCP Secret Manager
from google.cloud import secretmanager
client = secretmanager.SecretManagerServiceClient()
password = client.access_secret_version(name="projects/.../secrets/db-password")
```

### Network Security

**Atual:**
- Docker network isolada
- Apenas portas 8080 e 5432 expostas ao host
- GCS via HTTPS (TLS 1.2+)

**Recomendado (Prod):**
- VPC privada no GCP
- Cloud SQL proxy para PostgreSQL
- Private Service Connect para GCS
- Firewall rules restritivas

### Data Security

**Em TrÃ¢nsito:**
- GCS uploads: HTTPS (TLS 1.3)
- PostgreSQL: nÃ£o encriptado dentro da rede Docker
- RecomendaÃ§Ã£o prod: SSL/TLS para conexÃµes PostgreSQL

**Em Repouso:**
- PostgreSQL: sem encryption (filesystem default)
- GCS: encryption at rest padrÃ£o do GCP (AES-256)
- RecomendaÃ§Ã£o: habilitar Transparent Data Encryption (TDE)

**PII e Compliance:**
- Dataset Olist: sem PII (customer_id = UUID)
- NÃ£o aplicÃ¡vel: LGPD/GDPR
- RecomendaÃ§Ã£o: para dados reais, implementar tokenizaÃ§Ã£o/masking

---

## ğŸ“ˆ ESCALABILIDADE

### Escalabilidade Horizontal

**Airflow:**
- Atual: LocalExecutor (single node)
- PrÃ³ximo passo: CeleryExecutor com Redis
- Futuro: KubernetesExecutor (pods on-demand)

**PostgreSQL:**
- Atual: Single instance
- PrÃ³ximo passo: Read replicas
- Futuro: Cloud SQL HA com failover automÃ¡tico

**GCS:**
- JÃ¡ escalÃ¡vel infinitamente (managed service)
- Rate limits: 5000 writes/s, 20000 reads/s por bucket

### Escalabilidade Vertical

**Recursos AjustÃ¡veis:**

```yaml
# docker-compose.yml
services:
  airflow-webserver:
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
```

**ParÃ¢metros PostgreSQL:**

```sql
-- Ajustar para workload maior
ALTER SYSTEM SET shared_buffers = '2GB';
ALTER SYSTEM SET effective_cache_size = '6GB';
ALTER SYSTEM SET work_mem = '50MB';
ALTER SYSTEM SET max_connections = 200;
```

### Performance Optimization

**JÃ¡ Implementado:**
- Chunked inserts (1000 registros/batch)
- Parquet com compressÃ£o Snappy
- Ãndices em colunas de filtro/join
- Great Expectations com LIMIT 10k

**PrÃ³ximos Passos:**
- Particionamento de tabelas por data
- Incremental loads (apenas novos dados)
- Connection pooling (pgbouncer)
- Materialized views para agregaÃ§Ãµes

---

## ğŸ”„ DISASTER RECOVERY

### Backup Strategy

**PostgreSQL:**

Atual:
- Volume Docker com dados persistidos
- Backup manual: `docker exec postgres pg_dump`

Recomendado:
```bash
# Backup automatizado (cron diÃ¡rio)
pg_dump -h postgres -U airflow airflow > backup_$(date +%Y%m%d).sql

# Retention: 7 dias locais, 30 dias em GCS
gsutil cp backup_*.sql gs://olist-backups/postgres/
```

**GCS:**
- Versionamento de objetos habilitado
- Lifecycle policy: mover para Nearline apÃ³s 30 dias
- RetenÃ§Ã£o: 1 ano

**Great Expectations:**
- Arquivos versionados no Git
- Backup automÃ¡tico via volume Docker
- Data Docs regenerÃ¡veis a qualquer momento

### Recovery Time Objective (RTO)

| Componente | RTO Atual | RTO Prod Recomendado |
|------------|-----------|----------------------|
| Airflow Webserver | 5 minutos (restart container) | 2 minutos (auto-healing) |
| Airflow Scheduler | 5 minutos (restart container) | 1 minuto (standby replica) |
| PostgreSQL | 10 minutos (restore backup) | 5 minutos (failover automÃ¡tico) |
| GCS | N/A (managed, 99.9% SLA) | N/A |

### Recovery Point Objective (RPO)

| Dado | RPO Atual | RPO Prod Recomendado |
|------|-----------|----------------------|
| Metadados Airflow | 24 horas (backup diÃ¡rio) | 1 hora (streaming replication) |
| Dados olist_raw | 0 (reprocessÃ¡vel dos CSVs) | 0 (reprocessÃ¡vel) |
| GCS bronze | 0 (versioned) | 0 (versioned) |
| Great Expectations config | 0 (Git) | 0 (Git) |

### Disaster Scenarios

**Scenario 1: Container crash**
- Impacto: ServiÃ§o indisponÃ­vel
- Recovery: `docker-compose restart`
- Tempo: ~1 minuto
- Perda de dados: nenhuma (volumes persistidos)

**Scenario 2: CorrupÃ§Ã£o de dados PostgreSQL**
- Impacto: Queries falham
- Recovery: Restore do Ãºltimo backup + reprocessar DAGs
- Tempo: ~30 minutos
- Perda de dados: atÃ© 24h de metadados Airflow

**Scenario 3: Perda de host machine**
- Impacto: Tudo offline
- Recovery: Provisionar novo host + restore backups
- Tempo: ~2 horas
- Perda de dados: atÃ© 24h de metadados; 0 para dados (CSVs + GCS)

**Scenario 4: ExclusÃ£o acidental de bucket GCS**
- Impacto: Bronze layer perdido
- Recovery: Reprocessar DAG `04_ingest_csv_to_gcs`
- Tempo: ~10 minutos
- Perda de dados: nenhuma (CSVs originais existem)

---

## ğŸ“Š DIAGRAMAS TÃ‰CNICOS

### Entity Relationship Diagram (ERD)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CUSTOMERS     â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ PK customer_id  â”‚â”€â”€â”€â”
â”‚    cust_unique  â”‚   â”‚
â”‚    zip_code     â”‚   â”‚
â”‚    city         â”‚   â”‚
â”‚    state        â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                      â”‚ 1:N
                      â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     ORDERS      â”‚
                â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                â”‚ PK order_id     â”‚â”€â”€â”€â”
                â”‚ FK customer_id  â”‚   â”‚
                â”‚    status       â”‚   â”‚ 1:N
                â”‚    purchase_ts  â”‚   â”‚
                â”‚    delivered_ts â”‚   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                      â”‚               â”‚
                      â”‚ 1:N           â”‚
                      â–¼               â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  ORDER_PAYMENTS â”‚  â”‚   ORDER_REVIEWS  â”‚
            â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
            â”‚PK order_id      â”‚  â”‚PK review_id      â”‚
            â”‚PK payment_seq   â”‚  â”‚FK order_id       â”‚
            â”‚   type          â”‚  â”‚   score (1-5)    â”‚
            â”‚   installments  â”‚  â”‚   comment        â”‚
            â”‚   value         â”‚  â”‚   creation_date  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   ORDER_ITEMS   â”‚
                â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                â”‚PK order_id      â”‚â”€â”€â”€â”
                â”‚PK item_id       â”‚   â”‚
                â”‚FK product_id    â”‚â”€â”€â”€â”¼â”€â”€â”
                â”‚FK seller_id     â”‚â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”
                â”‚   price         â”‚   â”‚  â”‚  â”‚
                â”‚   freight       â”‚   â”‚  â”‚  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚  â”‚
                      â”‚               â”‚  â”‚  â”‚
                      â”‚               â”‚  â”‚  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    PRODUCTS     â”‚â—„â”€â”€â”˜  â”‚   SELLERS     â”‚â—„â”€â”˜  â”‚   GEOLOCATION   â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ PK product_id   â”‚      â”‚PK seller_id   â”‚      â”‚PK zip_code_pref â”‚
â”‚    category     â”‚      â”‚   zip_code    â”‚      â”‚PK lat           â”‚
â”‚    name_length  â”‚      â”‚   city        â”‚      â”‚PK lng           â”‚
â”‚    weight       â”‚      â”‚   state       â”‚      â”‚   city          â”‚
â”‚    dimensions   â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   state         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DAG Dependency Graph

```
Fase 1 - Setup (One-time)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 00_test_gcs_connection   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 04_create_schema         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Ingestion (Paralelo)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ 05_customers   â”‚  â”‚ 06_sellers     â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ 07_products    â”‚  â”‚ 12_geolocation â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 08_orders                â”‚ (depende: customers)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Items/Payments/Reviews (Paralelo)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ 09_order_items â”‚  â”‚ 10_payments    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚  â”‚ 11_reviews     â”‚  (depende: orders)             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 04_ingest_csv_to_gcs     â”‚ (paralelo com PostgreSQL)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 05_validate_data_quality â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Great Expectations (One-time Setup)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ 06_setup_great_expectationsâ”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚            â”‚                                         â”‚
â”‚            â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ 07_create_expectation_suitesâ”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Great Expectations (Recorrente)            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ 08_run_ge_validations      â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚            â”‚                                         â”‚
â”‚            â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ 09_force_build_data_docs   â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Diagram (DFD) - NÃ­vel 1

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CSV Files   â”‚
â”‚  (Source)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                      â”‚                        â”‚
       â–¼                      â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Airflow    â”‚    â”‚   Airflow    â”‚        â”‚   Airflow    â”‚
â”‚   Validate   â”‚    â”‚   Transform  â”‚        â”‚   Load GCS   â”‚
â”‚              â”‚    â”‚              â”‚        â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                        â”‚
       â”‚ âœ… OK             â”‚                        â”‚
       â–¼                   â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL  â”‚    â”‚  PostgreSQL  â”‚        â”‚     GCS      â”‚
â”‚  (olist_raw) â”‚â—„â”€â”€â”€â”¤  (olist_raw) â”‚        â”‚  (bronze/)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Great Expect. â”‚
â”‚  Validations â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Docs   â”‚
â”‚   (HTML)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” MONITORAMENTO E OBSERVABILIDADE

### MÃ©tricas Atuais

**Airflow UI:**
- Status de DAGs (success/failed/running)
- DuraÃ§Ã£o de tasks
- Logs detalhados
- Gantt chart de execuÃ§Ã£o

**PostgreSQL:**
```sql
-- Query para monitorar tamanho das tabelas
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE schemaname = 'olist_raw'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
```

**Great Expectations:**
- Data Docs HTML (success rate, expectations detalhadas)
- Validation results JSON
- Historical trend (manual via validations files)

### Alertas (Futuro)

**Recomendado implementar:**
- Slack webhook quando DAG falha
- Email quando Great Expectations < 80% success
- CloudWatch/Prometheus metrics
- Grafana dashboards

---

## ğŸ“š REFERÃŠNCIAS ARQUITETURAIS

### PadrÃµes Utilizados

1. **Medallion Architecture**
   - Bronze/Silver/Gold layers
   - Origem: Databricks

2. **ELT (Extract, Load, Transform)**
   - Load first, transform later
   - Vantagem: auditabilidade, reprocessamento

3. **Idempotency**
   - DAGs podem ser re-executadas sem side effects
   - TRUNCATE + INSERT vs UPSERT

4. **Schema-on-Read**
   - GCS bronze layer sem schema enforcement
   - Schema aplicado na leitura (silver layer)

### Trade-offs Arquiteturais

| DecisÃ£o | PrÃ³s | Contras |
|---------|------|---------|
| LocalExecutor vs CeleryExecutor | Simples, sem Redis | NÃ£o escala horizontalmente |
| PostgreSQL transacional | ACID, relacional | NÃ£o otimizado para analytics |
| Parquet no GCS | Eficiente, portÃ¡vel | Overhead de conversÃ£o |
| Great Expectations | Observabilidade | Complexidade adicional |
| Docker Compose | ReproduzÃ­vel, local | NÃ£o cloud-native |

---

## ğŸ“ CONCLUSÃƒO

A arquitetura do Olist Data Platform estabelece uma fundaÃ§Ã£o sÃ³lida para crescimento futuro. Os princÃ­pios de modularidade, observabilidade e simplicidade permitem evoluÃ§Ã£o incremental sem rewrites completos.

**PrÃ³ximas EvoluÃ§Ãµes Arquiteturais:**
1. MigraÃ§Ã£o para Cloud Composer (Airflow gerenciado)
2. ImplementaÃ§Ã£o de camadas Silver/Gold
3. CI/CD com testes automatizados
4. Monitoring avanÃ§ado (Prometheus + Grafana)
5. Incremental loads e CDC

---

**Ãšltima atualizaÃ§Ã£o:** 29 de Janeiro de 2025  
**Autor:** Hyego Jarllys  
**VersÃ£o:** 1.0  
**Status:** Aprovado para ProduÃ§Ã£o MVP
