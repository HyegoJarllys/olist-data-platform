"""
DAG: IngestÃ£o de Orders no PostgreSQL
Fase 1 - Data Ingestion
Autor: Hyego
Data: 2025-01-28
ATENÃ‡ÃƒO: Tabela com FK para customers e mÃºltiplos timestamps
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import logging
from sqlalchemy import create_engine, text

# ConfiguraÃ§Ãµes
CSV_PATH = '/opt/airflow/data/raw/olist_orders_dataset.csv'
TABLE_NAME = 'olist_raw.orders'

# ConexÃ£o direta
DB_CONNECTION = 'postgresql://airflow:airflow@postgres:5432/airflow'

default_args = {
    'owner': 'hyego',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}


def validate_csv():
    """Valida estrutura e qualidade do CSV"""
    logger = logging.getLogger(__name__)
    
    try:
        # Ler CSV
        df = pd.read_csv(CSV_PATH)
        logger.info(f"âœ… CSV carregado: {len(df)} registros")
        
        # Validar colunas esperadas
        expected_cols = [
            'order_id',
            'customer_id',
            'order_status',
            'order_purchase_timestamp',
            'order_approved_at',
            'order_delivered_carrier_date',
            'order_delivered_customer_date',
            'order_estimated_delivery_date'
        ]
        
        missing_cols = set(expected_cols) - set(df.columns)
        if missing_cols:
            raise ValueError(f"âŒ Colunas faltando: {missing_cols}")
        
        logger.info(f"âœ… Colunas validadas: {list(df.columns)}")
        
        # Validar valores nulos em PK
        null_pks = df['order_id'].isnull().sum()
        if null_pks > 0:
            raise ValueError(f"âŒ {null_pks} PKs nulas encontradas!")
        
        # Validar valores nulos em FK
        null_fks = df['customer_id'].isnull().sum()
        if null_fks > 0:
            raise ValueError(f"âŒ {null_fks} FKs nulas encontradas!")
        
        logger.info("âœ… Sem PKs ou FKs nulas")
        
        # Validar duplicatas em PK
        duplicates = df['order_id'].duplicated().sum()
        if duplicates > 0:
            logger.warning(f"âš ï¸ {duplicates} duplicatas encontradas - serÃ£o removidas")
        
        # EstatÃ­sticas
        logger.info(f"""
        ğŸ“Š ESTATÃSTICAS DO CSV:
        - Total registros: {len(df)}
        - Orders Ãºnicos: {df['order_id'].nunique()}
        - Customers Ãºnicos: {df['customer_id'].nunique()}
        - Status Ãºnicos: {df['order_status'].unique().tolist()}
        - Valores nulos por coluna:
        {df.isnull().sum().to_dict()}
        """)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Erro na validaÃ§Ã£o: {str(e)}")
        raise


def load_to_postgres():
    """Carrega dados do CSV para PostgreSQL"""
    logger = logging.getLogger(__name__)
    
    try:
        # Ler CSV
        df = pd.read_csv(CSV_PATH)
        logger.info(f"ğŸ“‚ CSV carregado: {len(df)} registros")
        
        # Converter colunas de data para datetime
        date_columns = [
            'order_purchase_timestamp',
            'order_approved_at',
            'order_delivered_carrier_date',
            'order_delivered_customer_date',
            'order_estimated_delivery_date'
        ]
        
        for col in date_columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
            logger.info(f"âœ… Convertido {col} para datetime")
        
        # Remover duplicatas (manter primeira ocorrÃªncia)
        original_len = len(df)
        df = df.drop_duplicates(subset=['order_id'], keep='first')
        removed = original_len - len(df)
        if removed > 0:
            logger.warning(f"ğŸ—‘ï¸ {removed} duplicatas removidas")
        
        # Conectar ao PostgreSQL
        logger.info("ğŸ”Œ Conectando ao PostgreSQL...")
        engine = create_engine(DB_CONNECTION)
        
        # Truncar tabela
        with engine.begin() as conn:
            conn.execute(text(f"TRUNCATE TABLE {TABLE_NAME} CASCADE"))
            logger.info(f"ğŸ—‘ï¸ Tabela {TABLE_NAME} truncada")
        
        # Inserir dados
        logger.info(f"ğŸ“ Inserindo {len(df)} registros...")
        df.to_sql(
            name='orders',
            con=engine,
            schema='olist_raw',
            if_exists='append',
            index=False,
            method='multi',
            chunksize=1000
        )
        
        logger.info(f"âœ… {len(df)} registros inseridos em {TABLE_NAME}")
        
        # Validar contagem
        with engine.connect() as conn:
            result = conn.execute(text(f"SELECT COUNT(*) FROM {TABLE_NAME}"))
            count = result.scalar()
            logger.info(f"âœ… ValidaÃ§Ã£o: {count} registros na tabela")
            
            if count != len(df):
                raise ValueError(f"âŒ Contagem divergente! CSV: {len(df)}, DB: {count}")
        
        logger.info("ğŸ‰ IngestÃ£o concluÃ­da com sucesso!")
        return count
        
    except Exception as e:
        logger.error(f"âŒ Erro na ingestÃ£o: {str(e)}")
        raise


def validate_data_quality():
    """Valida qualidade dos dados inseridos"""
    logger = logging.getLogger(__name__)
    
    try:
        # Conectar ao PostgreSQL
        engine = create_engine(DB_CONNECTION)
        
        # Query de validaÃ§Ã£o
        validation_query = text("""
        SELECT 
            COUNT(*) as total_records,
            COUNT(DISTINCT order_id) as unique_orders,
            COUNT(DISTINCT customer_id) as unique_customers,
            COUNT(CASE WHEN order_id IS NULL THEN 1 END) as null_pks,
            COUNT(CASE WHEN customer_id IS NULL THEN 1 END) as null_fks,
            COUNT(CASE WHEN order_status = 'delivered' THEN 1 END) as delivered_orders,
            COUNT(CASE WHEN order_delivered_customer_date IS NULL THEN 1 END) as null_delivery_dates
        FROM olist_raw.orders
        """)
        
        with engine.connect() as conn:
            result = conn.execute(validation_query).fetchone()
        
        logger.info(f"""
        ğŸ“Š VALIDAÃ‡ÃƒO DE QUALIDADE:
        - Total registros: {result[0]}
        - Orders Ãºnicos: {result[1]}
        - Customers Ãºnicos: {result[2]}
        - PKs nulas: {result[3]}
        - FKs nulas: {result[4]}
        - Orders entregues: {result[5]}
        - Datas de entrega nulas: {result[6]}
        """)
        
        # ValidaÃ§Ãµes crÃ­ticas
        if result[3] > 0:
            raise ValueError(f"âŒ {result[3]} PKs nulas encontradas!")
        
        if result[4] > 0:
            raise ValueError(f"âŒ {result[4]} FKs nulas encontradas!")
        
        if result[0] != result[1]:
            logger.warning(f"âš ï¸ Total ({result[0]}) != Ãšnicos ({result[1]})")
        
        # Validar integridade referencial
        fk_validation = text("""
        SELECT COUNT(*) 
        FROM olist_raw.orders o
        LEFT JOIN olist_raw.customers c ON o.customer_id = c.customer_id
        WHERE c.customer_id IS NULL
        """)
        
        with engine.connect() as conn:
            orphan_count = conn.execute(fk_validation).scalar()
        
        if orphan_count > 0:
            raise ValueError(f"âŒ {orphan_count} orders com customer_id invÃ¡lido!")
        
        logger.info("âœ… Integridade referencial validada!")
        logger.info("âœ… ValidaÃ§Ã£o de qualidade aprovada!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Erro na validaÃ§Ã£o: {str(e)}")
        raise


# DAG Definition
with DAG(
    dag_id='08_ingest_orders',
    default_args=default_args,
    description='IngestÃ£o de orders (CSV â†’ PostgreSQL)',
    schedule_interval=None,
    start_date=datetime(2025, 1, 28),
    catchup=False,
    tags=['fase-1', 'ingestion', 'postgresql', 'orders'],
) as dag:
    
    # Task 1: Validar CSV
    task_validate_csv = PythonOperator(
        task_id='validate_csv',
        python_callable=validate_csv,
    )
    
    # Task 2: Carregar dados
    task_load_data = PythonOperator(
        task_id='load_to_postgres',
        python_callable=load_to_postgres,
    )
    
    # Task 3: Validar qualidade
    task_validate_quality = PythonOperator(
        task_id='validate_data_quality',
        python_callable=validate_data_quality,
    )
    
    # Pipeline
    task_validate_csv >> task_load_data >> task_validate_quality
